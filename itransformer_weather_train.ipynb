{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd11113-b3e6-4b70-810c-e1b95ec0c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from experiments.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
    "from experiments.exp_long_term_forecasting_partial import Exp_Long_Term_Forecast_Partial\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "359f3247-f786-4789-92cb-8cc7a35d8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed = 2023\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='iTransformer')\n",
    "\n",
    "\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='weather_96_96', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='iTransformer',\n",
    "                    help='model name, options: [iTransformer, iInformer, iReformer, iFlowformer, iFlashformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='custom', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./iTransformer/iTransformer_datasets/weather/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='weather.csv', help='data csv file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length') # no longer needed in inverted Transformers\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "# model define\n",
    "parser.add_argument('--enc_in', type=int, default=21, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=21, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=21, help='output size') # applicable on arbitrary number of variates in inverted Transformers\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=512, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, \n",
    "                    default='Exp',\n",
    "                    # default='test', \n",
    "                    help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='MSE', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "\n",
    "# iTransformer\n",
    "parser.add_argument('--exp_name', type=str, required=False, default='MTSF',\n",
    "                    help='experiemnt name, options:[MTSF, partial_train]')\n",
    "parser.add_argument('--channel_independence', type=bool, default=False, help='whether to use channel_independence mechanism')\n",
    "parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n",
    "parser.add_argument('--class_strategy', type=str, default='projection', help='projection/average/cls_token')\n",
    "parser.add_argument('--target_root_path', type=str, default='./data/electricity/', help='root path of the data file')\n",
    "parser.add_argument('--target_data_path', type=str, default='electricity.csv', help='data file')\n",
    "parser.add_argument('--efficient_training', type=bool, default=False, help='whether to use efficient_training (exp_name should be partial train)') # See Figure 8 of our paper for the detail\n",
    "parser.add_argument('--use_norm', type=int, default=True, help='use norm and denorm')\n",
    "parser.add_argument('--partial_start_index', type=int, default=0, help='the start index of variates for partial training, '\n",
    "                                                                       'you can select [partial_start_index, min(enc_in + partial_start_index, N)]')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60004e5b-63b1-48b9-aedd-63125e4a9032",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c605e4a3-7ed9-421f-be9c-3b24e812efd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "850761f6-1781-47b6-aafa-7b78acc43210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(is_training=1, model_id='weather_96_96', model='iTransformer', data='custom', root_path='./iTransformer_datasets/weather/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, enc_in=21, dec_in=21, c_out=21, d_model=512, n_heads=8, e_layers=3, d_layers=1, d_ff=512, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3', exp_name='MTSF', channel_independence=False, inverse=False, class_strategy='projection', target_root_path='./data/electricity/', target_data_path='electricity.csv', efficient_training=False, use_norm=True, partial_start_index=0)\n"
     ]
    }
   ],
   "source": [
    "if args.use_gpu and args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(' ', '')\n",
    "        device_ids = args.devices.split(',')\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddbbc5f3-f70f-4d53-b82c-7b6821b6035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.exp_name == 'partial_train': # See Figure 8 of our paper, for the detail\n",
    "    Exp = Exp_Long_Term_Forecast_Partial\n",
    "else: # MTSF: multivariate time series forecasting\n",
    "    Exp = Exp_Long_Term_Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9870e5e3-f0c9-4379-971c-a3f73a1000e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : weather_96_96_iTransformer_custom_M_ft96_sl48_ll96_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 36696\n",
      "val 5175\n",
      "test 10444\n",
      "\titers: 100, epoch: 1 | loss: 0.4491155\n",
      "\tspeed: 0.3392s/iter; left time: 3853.2242s\n",
      "\titers: 200, epoch: 1 | loss: 0.5053276\n",
      "\tspeed: 0.2842s/iter; left time: 3200.9071s\n",
      "\titers: 300, epoch: 1 | loss: 0.3332857\n",
      "\tspeed: 0.2821s/iter; left time: 3148.2782s\n",
      "\titers: 400, epoch: 1 | loss: 0.4098361\n",
      "\tspeed: 0.2828s/iter; left time: 3128.1341s\n",
      "\titers: 500, epoch: 1 | loss: 0.5137857\n",
      "\tspeed: 0.2808s/iter; left time: 3077.7411s\n",
      "\titers: 600, epoch: 1 | loss: 0.3172986\n",
      "\tspeed: 0.2807s/iter; left time: 3049.1172s\n",
      "\titers: 700, epoch: 1 | loss: 0.5936196\n",
      "\tspeed: 0.2812s/iter; left time: 3026.0540s\n",
      "\titers: 800, epoch: 1 | loss: 0.4951524\n",
      "\tspeed: 0.2800s/iter; left time: 2985.0147s\n",
      "\titers: 900, epoch: 1 | loss: 0.5038437\n",
      "\tspeed: 0.2809s/iter; left time: 2966.4557s\n",
      "\titers: 1000, epoch: 1 | loss: 0.3140152\n",
      "\tspeed: 0.2807s/iter; left time: 2936.3261s\n",
      "\titers: 1100, epoch: 1 | loss: 0.5296207\n",
      "\tspeed: 0.2820s/iter; left time: 2921.9465s\n",
      "Epoch: 1 cost time: 330.7199683189392\n",
      "Epoch: 1, Steps: 1146 | Train Loss: 0.4974456 Vali Loss: 0.4366845 Test Loss: 0.1832981\n",
      "Validation loss decreased (inf --> 0.436685).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4107980\n",
      "\tspeed: 1.7225s/iter; left time: 17594.9112s\n",
      "\titers: 200, epoch: 2 | loss: 0.3063270\n",
      "\tspeed: 0.2825s/iter; left time: 2857.1402s\n",
      "\titers: 300, epoch: 2 | loss: 0.3874223\n",
      "\tspeed: 0.2814s/iter; left time: 2818.2050s\n",
      "\titers: 400, epoch: 2 | loss: 0.4288125\n",
      "\tspeed: 0.2823s/iter; left time: 2799.0716s\n",
      "\titers: 500, epoch: 2 | loss: 0.3249929\n",
      "\tspeed: 0.2819s/iter; left time: 2767.2096s\n",
      "\titers: 600, epoch: 2 | loss: 1.0208750\n",
      "\tspeed: 0.2797s/iter; left time: 2717.5791s\n",
      "\titers: 700, epoch: 2 | loss: 0.3193903\n",
      "\tspeed: 0.2828s/iter; left time: 2719.0315s\n",
      "\titers: 800, epoch: 2 | loss: 0.4259833\n",
      "\tspeed: 0.2818s/iter; left time: 2681.1311s\n",
      "\titers: 900, epoch: 2 | loss: 0.3072714\n",
      "\tspeed: 0.2834s/iter; left time: 2668.4381s\n",
      "\titers: 1000, epoch: 2 | loss: 0.6402925\n",
      "\tspeed: 0.2825s/iter; left time: 2631.4046s\n",
      "\titers: 1100, epoch: 2 | loss: 0.3438151\n",
      "\tspeed: 0.2797s/iter; left time: 2577.4435s\n",
      "Epoch: 2 cost time: 326.433557510376\n",
      "Epoch: 2, Steps: 1146 | Train Loss: 0.4496772 Vali Loss: 0.4261753 Test Loss: 0.1776872\n",
      "Validation loss decreased (0.436685 --> 0.426175).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.3711359\n",
      "\tspeed: 1.6509s/iter; left time: 14971.8384s\n",
      "\titers: 200, epoch: 3 | loss: 0.3132005\n",
      "\tspeed: 0.2735s/iter; left time: 2452.7817s\n",
      "\titers: 300, epoch: 3 | loss: 0.5799095\n",
      "\tspeed: 0.2737s/iter; left time: 2427.2298s\n",
      "\titers: 400, epoch: 3 | loss: 0.4667489\n",
      "\tspeed: 0.2731s/iter; left time: 2394.9756s\n",
      "\titers: 500, epoch: 3 | loss: 0.3376468\n",
      "\tspeed: 0.2712s/iter; left time: 2350.6346s\n",
      "\titers: 600, epoch: 3 | loss: 0.3141442\n",
      "\tspeed: 0.2714s/iter; left time: 2325.7043s\n",
      "\titers: 700, epoch: 3 | loss: 0.2923342\n",
      "\tspeed: 0.2731s/iter; left time: 2312.4624s\n",
      "\titers: 800, epoch: 3 | loss: 0.6105658\n",
      "\tspeed: 0.2738s/iter; left time: 2291.7651s\n",
      "\titers: 900, epoch: 3 | loss: 0.4731675\n",
      "\tspeed: 0.2732s/iter; left time: 2259.1243s\n",
      "\titers: 1000, epoch: 3 | loss: 0.3016504\n",
      "\tspeed: 0.2743s/iter; left time: 2240.4933s\n",
      "\titers: 1100, epoch: 3 | loss: 0.3312116\n",
      "\tspeed: 0.2722s/iter; left time: 2196.7077s\n",
      "Epoch: 3 cost time: 315.6119170188904\n",
      "Epoch: 3, Steps: 1146 | Train Loss: 0.4269795 Vali Loss: 0.4362256 Test Loss: 0.1776616\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2895018\n",
      "\tspeed: 1.6125s/iter; left time: 12775.4730s\n",
      "\titers: 200, epoch: 4 | loss: 0.2292689\n",
      "\tspeed: 0.2724s/iter; left time: 2130.8291s\n",
      "\titers: 300, epoch: 4 | loss: 0.3113969\n",
      "\tspeed: 0.2714s/iter; left time: 2095.6930s\n",
      "\titers: 400, epoch: 4 | loss: 0.4236707\n",
      "\tspeed: 0.2724s/iter; left time: 2076.3405s\n",
      "\titers: 500, epoch: 4 | loss: 0.2843925\n",
      "\tspeed: 0.2731s/iter; left time: 2054.5619s\n",
      "\titers: 600, epoch: 4 | loss: 0.3694341\n",
      "\tspeed: 0.2715s/iter; left time: 2015.5763s\n",
      "\titers: 700, epoch: 4 | loss: 0.2560661\n",
      "\tspeed: 0.2722s/iter; left time: 1992.9617s\n",
      "\titers: 800, epoch: 4 | loss: 0.3159940\n",
      "\tspeed: 0.2713s/iter; left time: 1959.5689s\n",
      "\titers: 900, epoch: 4 | loss: 0.4191686\n",
      "\tspeed: 0.2720s/iter; left time: 1937.7097s\n",
      "\titers: 1000, epoch: 4 | loss: 0.3825012\n",
      "\tspeed: 0.2738s/iter; left time: 1922.5548s\n",
      "\titers: 1100, epoch: 4 | loss: 0.3786490\n",
      "\tspeed: 0.2715s/iter; left time: 1879.4234s\n",
      "Epoch: 4 cost time: 314.6269121170044\n",
      "Epoch: 4, Steps: 1146 | Train Loss: 0.4143799 Vali Loss: 0.4343638 Test Loss: 0.1749612\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4560075\n",
      "\tspeed: 1.6163s/iter; left time: 10953.4942s\n",
      "\titers: 200, epoch: 5 | loss: 0.3710010\n",
      "\tspeed: 0.2726s/iter; left time: 1820.2530s\n",
      "\titers: 300, epoch: 5 | loss: 0.3136957\n",
      "\tspeed: 0.2731s/iter; left time: 1796.0386s\n",
      "\titers: 400, epoch: 5 | loss: 0.3135441\n",
      "\tspeed: 0.2729s/iter; left time: 1767.8513s\n",
      "\titers: 500, epoch: 5 | loss: 0.2782956\n",
      "\tspeed: 0.2746s/iter; left time: 1751.0544s\n",
      "\titers: 600, epoch: 5 | loss: 0.3472484\n",
      "\tspeed: 0.2731s/iter; left time: 1714.1875s\n",
      "\titers: 700, epoch: 5 | loss: 0.3163032\n",
      "\tspeed: 0.2727s/iter; left time: 1684.6084s\n",
      "\titers: 800, epoch: 5 | loss: 0.2570043\n",
      "\tspeed: 0.2727s/iter; left time: 1656.9498s\n",
      "\titers: 900, epoch: 5 | loss: 0.2436285\n",
      "\tspeed: 0.2736s/iter; left time: 1635.4282s\n",
      "\titers: 1000, epoch: 5 | loss: 0.2440798\n",
      "\tspeed: 0.2734s/iter; left time: 1606.6996s\n",
      "\titers: 1100, epoch: 5 | loss: 0.2384326\n",
      "\tspeed: 0.2724s/iter; left time: 1573.6298s\n",
      "Epoch: 5 cost time: 316.07571029663086\n",
      "Epoch: 5, Steps: 1146 | Train Loss: 0.4073157 Vali Loss: 0.4321491 Test Loss: 0.1745719\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : weather_96_96_iTransformer_custom_M_ft96_sl48_ll96_pl512_dm8_nh3_el1_dl512_df1_fctimeF_ebTrue_dtExp_projection_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 10444\n",
      "test shape: (10444, 1, 96, 21) (10444, 1, 96, 21)\n",
      "test shape: (10444, 96, 21) (10444, 96, 21)\n",
      "mse:0.17768724262714386, mae:0.21875596046447754\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des,\n",
    "        args.class_strategy, ii)\n",
    "\n",
    "    exp = Exp(args)  # set experiments\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "\n",
    "    if args.do_predict:\n",
    "        print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.predict(setting, True)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b287b1d-47dd-445b-9058-29d132bed6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
